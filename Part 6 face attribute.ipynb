{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py:2379: FutureWarning: split() requires a non-empty pattern match.\n",
      "  yield pat.split(line.strip())\n",
      "/home/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py:2381: FutureWarning: split() requires a non-empty pattern match.\n",
      "  yield pat.split(line.strip())\n",
      "/home/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./data/list_attr_celeba.txt', sep=\"\\s*\", header=0)\n",
    "partition = pd.read_csv('./data/list_eval_partition.txt', sep=\"\\s*\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition.columns = [\"filename\", \"file_cate\"]\n",
    "data = data.merge(partition, on = \"filename\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['filename', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n",
      "       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n",
      "       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows',\n",
      "       'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n",
      "       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
      "       'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',\n",
      "       'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns',\n",
      "       'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n",
      "       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',\n",
      "       'Wearing_Necktie', 'Young', 'file_cate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./data/list_merge_attr_celeba.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')#将图片转换为RGB格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_data1=pil_loader(r'/home/tongqing.li/qianbao/picture_score/data/img_align_celeba/000001.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 178, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(picture_data1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader \n",
    "import os\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, loadPath, transform, data_category = 0): #r'home/tongqing.li/lesson/pytorch/MNIST_data/MNIST/processed/training.pt'\n",
    "        super(myDataset, self).__init__()\n",
    "        self.data = pd.read_csv(loadPath)\n",
    "        self.data = self.data[self.data[\"file_cate\"] == data_category]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #Heavy_Makeup，Male，Wearing_Earrings，Wearing_Necklace，Wearing_Necktie\n",
    "        picture = pil_loader(r'/home/tongqing.li/qianbao/picture_score/data/img_align_celeba/' + self.data.iloc[index][\"filename\"])\n",
    "        label = self.data.iloc[index][[\"Eyeglasses\", \"Heavy_Makeup\", \"Male\", \"Wearing_Earrings\", \"Wearing_Necklace\", \"Wearing_Necktie\"]]\n",
    "        for i in [\"Eyeglasses\", \"Heavy_Makeup\", \"Male\", \"Wearing_Earrings\", \"Wearing_Necklace\", \"Wearing_Necktie\"]:\n",
    "            if (label.loc[i] == -1):\n",
    "                label.loc[i] = 0\n",
    "        label = torch.tensor(label)\n",
    "        picture = self.transform(picture)\n",
    "        return (picture, label.long())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "transform = transforms.Compose([transforms.Resize((224,224)), transforms.CenterCrop(224),transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_face_dataset = myDataset(\"./data/list_merge_attr_celeba.csv\", data_category = 0, transform = transform)\n",
    "valid_face_dataset = myDataset(\"./data/list_merge_attr_celeba.csv\", data_category = 1, transform = transform)\n",
    "test_face_dataset = myDataset(\"./data/list_merge_attr_celeba.csv\", data_category = 2, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_face_dataset.__getitem__(0)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_face_loader = DataLoader(train_face_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers, pin_memory=True)\n",
    "valid_face_loader = DataLoader(valid_face_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers, pin_memory=True)\n",
    "test_face_loader = DataLoader(test_face_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3)\n",
    "        # an affine operation: y = Wx + b \n",
    "        self.fc1 = nn.Linear(2048, 84)\n",
    "        self.fc2 = nn.Linear(84, 2)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "        self.fc4 = nn.Linear(84, 2)\n",
    "        self.fc5 = nn.Linear(84, 2)\n",
    "        self.fc6 = nn.Linear(84, 2)\n",
    "        self.fc7 = nn.Linear(84, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool1(F.relu(self.conv2(x)))\n",
    "        x = self.pool1(F.relu(self.conv3(x)))\n",
    "        x = self.pool1(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        y = []\n",
    "        y.append(self.fc2(x))\n",
    "        y.append(self.fc3(x))\n",
    "        y.append(self.fc4(x))\n",
    "        y.append(self.fc5(x))\n",
    "        y.append(self.fc6(x))\n",
    "        y.append(self.fc7(x))\n",
    "        return y\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100..  Training Loss: 0.648..  Test Loss: 0.004..  Test Accuracy: 0.948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-226-b0402c175312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#model = Net()\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "epochs = 100\n",
    "steps = 0\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_face_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(images)\n",
    "        #print(labels.shape)\n",
    "        loss = []\n",
    "        for i, log_ps_data in enumerate(log_ps):\n",
    "            loss.append(criterion(log_ps_data, labels[:,i]))\n",
    "        #print(loss)\n",
    "        sum(loss).backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += sum(loss).item()\n",
    "    \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()#这里就是关闭dropout了。\n",
    "            for images, labels in valid_face_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                log_ps = model(images)\n",
    "                test_loss = []\n",
    "                \n",
    "                for i, log_ps_data in enumerate(log_ps):\n",
    "                    test_loss.append(criterion(log_ps_data, labels[:,i]))\n",
    "                    top_p, top_class = log_ps_data.topk(1, dim=1)\n",
    "                    equals = top_class == labels[:, i].view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        #model.load_state_dict(torch.load(filepath))\n",
    "        torch.save(model.state_dict(), r'/home/tongqing.li/qianbao/deep_learning/neural_network_in_pytorch/deep_model/model' + str(e + 1))\n",
    "        train_losses.append(running_loss/len(train_face_loader))\n",
    "        test_losses.append(sum(test_loss)/len(valid_face_loader))\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_face_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "        with torch.no_grad():    \n",
    "            model.eval()#这里就是关闭dropout了。\n",
    "            accuracy = [0, 0, 0, 0, 0, 0]\n",
    "            for images, labels in valid_face_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                log_ps = model(images)\n",
    "                test_loss = []\n",
    "                x = 0\n",
    "                for i, log_ps_data in enumerate(log_ps):\n",
    "                    test_loss.append(criterion(log_ps_data, labels[:,i]))\n",
    "                    top_p, top_class = log_ps_data.topk(1, dim=1)\n",
    "                    equals = top_class == labels[:, i].view(*top_class.shape)\n",
    "                    accuracy[i] += torch.mean(equals.type(torch.FloatTensor))\n",
    "                #print(accuracy)\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9881)\n",
      "tensor(0.9012)\n",
      "tensor(0.9691)\n",
      "tensor(0.8914)\n",
      "tensor(0.8621)\n",
      "tensor(0.9532)\n",
      "tensor(0.9275)\n"
     ]
    }
   ],
   "source": [
    "tot = 0\n",
    "for i in range(6):\n",
    "    print(accuracy[i]/len(test_face_loader))\n",
    "    tot = tot + accuracy[i]/len(test_face_loader)\n",
    "print(tot/6)\n",
    "#\"Eyeglasses\", \"Heavy_Makeup\", \"Male\", \"Wearing_Earrings\", \"Wearing_Necklace\", \"Wearing_Necktie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/tongqing.li/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "Downloading: \"https://download.pytorch.org/models/vgg11-bbd30ac9.pth\" to /home/tongqing.li/.cache/torch/checkpoints/vgg11-bbd30ac9.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ed68242276443d978819d59f855514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=531456000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11', pretrained=True)\n",
    "# or any of these variants\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg11_bn', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg13', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg13_bn', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16_bn', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19_bn', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
